{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:34:24.327936971Z",
     "start_time": "2023-09-26T12:34:24.325168030Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from omnibelt import human_readable_number\n",
    "import torch\n",
    "torch.set_default_device('cuda')\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "\treturn sum(p.numel() for p in model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:34:24.360439328Z",
     "start_time": "2023-09-26T12:34:24.328533079Z"
    }
   },
   "id": "c1141c2583750dc2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:34:29.774662817Z",
     "start_time": "2023-09-26T12:34:27.120952911Z"
    }
   },
   "id": "81a3dc138e6bf2af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(human_readable_number(count_parameters(model)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fffe4cee0cb9656"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed:  34 tokens per second\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''Imagine a world where two distinct genres, such as cyberpunk and renaissance romance, have been seamlessly blended. Describe a day in the life of a character living in this unique world.''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "outputs = model.generate(**inputs, max_length=500)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time = start_event.elapsed_time(end_event)\n",
    "total_generated_tokens = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "print(f'Speed: {total_generated_tokens / elapsed_time * 1000: .2g} tps')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:47:20.143885762Z",
     "start_time": "2023-09-26T12:47:20.083413321Z"
    }
   },
   "id": "1345e471dbec93ee"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a world where two distinct genres, such as cyberpunk and renaissance romance, have been seamlessly blended. Describe a day in the life of a character living in this unique world.\n",
      "\n",
      "Answer: In this world, a character named Alex wakes up to the sound of a melodic symphony, composed entirely of futuristic electronic beats. As Alex steps outside, they are greeted by a breathtaking sight - a cityscape adorned with vibrant neon lights and towering skyscrapers. The streets are filled with people dressed in futuristic attire, their movements synchronized to the rhythm of the music.\n",
      "\n",
      "Alex's day begins with a visit to a futuristic art gallery, where they encounter a mesmerizing installation that combines elements of cyberpunk and renaissance romance. The artwork, created by a renowned artist, depicts a couple locked in an embrace, their bodies intertwined with intricate electronic patterns. The colors and textures evoke a sense of passion and longing, blurring the lines between the two genres.\n",
      "\n",
      "As the day progresses, Alex attends a futuristic music festival, where they witness a live performance that seamlessly blends cyberpunk and renaissance romance. The stage is transformed into a magical realm, with holographic projections of mythical creatures and ethereal landscapes. The music, composed by a talented artist, combines electronic beats with haunting melodies, creating a symphony that resonates with the audience's emotions.\n",
      "\n",
      "In the evening, Alex attends a grand gala, where they are treated to a feast of futuristic delicacies. The menu features dishes inspired by the flavors of the renaissance era, such as roasted peacock and truffle risotto. The combination of traditional and modern culinary techniques creates a unique dining experience that tantalizes the taste buds.\n",
      "\n",
      "As the night comes to a close, Alex reflects on the day's events, realizing that the blending of cyberpunk and renaissance romance has created a world where imagination knows no bounds. The fusion of these genres has allowed for the creation of a new form of art that captivates and inspires.\n",
      "\n",
      "Exercise 2:\n",
      "Think of a real-world example where two genres have been successfully blended to create something new and innovative. Describe the impact of this blending on the art world.\n",
      "\n",
      "Answer: One real-world example of the successful blending of genres is the emergence of street art. Street art, which originated in urban environments, has evolved over the years to incorporate various genres and styles. Artists have started using graffiti as a medium to express their creativity, often\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:47:41.699519734Z",
     "start_time": "2023-09-26T12:47:41.638473110Z"
    }
   },
   "id": "c65876043dac0c22"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "total_generated_tokens = len(outputs[0]) - len(inputs['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:47:00.393313931Z",
     "start_time": "2023-09-26T12:47:00.347626182Z"
    }
   },
   "id": "2513f0794c74a4e"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-24653,    243,   -495,   -310,   -234,  -6810, -27462,    489,   -384,\n           145,  -9575, -29854,    210, -48417, -19161,    489,     77,    -87,\n        -33181, -31563,    487, -38873,  -4392,    243,   -610,    213,    238,\n          -704,    214,    243,  -1595,  -2377,    213,     72,  -3248,   -495,\n           487], device='cuda:0')"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_generated_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:46:40.978272582Z",
     "start_time": "2023-09-26T12:46:40.913713749Z"
    }
   },
   "id": "1a846aa10c47fb74"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# out = pipeline('text-generation', model=\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "# out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T12:38:24.705233438Z",
     "start_time": "2023-09-26T12:38:24.663555698Z"
    }
   },
   "id": "c94e78ae888b34ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ce26cc9f3b2bccb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c5592413aa89a1f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 11:43:36.450276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 11:43:37.026330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:43:38.610512324Z",
     "start_time": "2023-09-27T09:43:34.693056958Z"
    }
   },
   "id": "2162dbfa77e336d4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-miniguanaco\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:43:38.616429122Z",
     "start_time": "2023-09-27T09:43:38.611648660Z"
    }
   },
   "id": "1d4bc00826128955"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6cfbfc4ad664fa297bfb2aaae656609"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "493eeb2e7d6047a58f9ac82bbf944c82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48c674a377a14ce899a52406aada3371"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db70177945744cdbb4774da9b5216711"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b58e989b26fd4590bf2eb18a1932d82e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8a3a44fb72a45b194be8a0356118a66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fac93b638294cfea516192fc7adee27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb107c5d7eed44e48a9c4d39d37d77bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8bdfddac0a645d880e52afa567cda99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7beefde20bb048e696962d62a67333c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03752c64391b4190a1b5f4b1a0e1fd15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e6dfed046f94636947b8248d9e418f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/miniconda3/envs/clones/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/fleeb/miniconda3/envs/clones/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f6f209e121949218e89560b1ba713f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59c0f8daffad4604a016901be979b3d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e9d110d1e4e4db79821601e6a8d8a48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9973e50af503483e94bc4aab9bcc54c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a661b38182fe433abec73d4e41306fb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:46:06.948763170Z",
     "start_time": "2023-09-27T09:43:43.521382488Z"
    }
   },
   "id": "d3f005f9b9eb0663"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/miniconda3/envs/clones/lib/python3.8/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/fleeb/miniconda3/envs/clones/lib/python3.8/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed:  3.3 tps\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''Imagine a world where two distinct genres, such as cyberpunk and renaissance romance, have been seamlessly blended. Describe a day in the life of a character living in this unique world.''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "outputs = model.generate(**inputs, max_length=500)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time = start_event.elapsed_time(end_event)\n",
    "total_generated_tokens = len(outputs[0]) - len(inputs['input_ids'][0])\n",
    "print(f'Speed: {total_generated_tokens / elapsed_time * 1000: .2g} tps')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:50:21.207147374Z",
     "start_time": "2023-09-27T09:48:05.384523514Z"
    }
   },
   "id": "d3ecc3e360207b69"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Imagine a world where two distinct genres, such as cyberpunk and renaissance romance, have been seamlessly blended. Describe a day in the life of a character living in this unique world.\n",
      "\n",
      "In the city of Cygnus, a sprawling metropolis nestled between towering mountains and a glimmering lake, the sun rises over the skyline, casting a warm orange glow over the bustling streets. The air is thick with the hum of holographic advertisements and the distant thrum of hovercraft engines.\n",
      "\n",
      "Our protagonist, a young woman named Aria, stirs in her cozy apartment, nestled in the heart of the city's lower levels. She rubs the sleep from her eyes and reaches for her cybernetic implant, a sleek, silver device that allows her to interface with the city's vast network of computers and databases.\n",
      "\n",
      "Aria is a data analyst, specializing in the retrieval and interpretation of historical records from the Renaissance era. Her work is in high demand, as the city's wealthy elite are eager to learn more about the distant past and how it may inform their present.\n",
      "\n",
      "As she sips her morning tea, Aria's implant beeps, signaling a new message from her client. She logs in and finds a request for information on a particular Renaissance-era noblewoman, known for her beauty and poetic talents. Aria sets to work, scouring the city's vast archives and databases for any mention of the woman, her family, or her works.\n",
      "\n",
      "As she works, Aria's thoughts drift to her own life. She is a cyborg, her body enhanced with cybernetic implants that allow her to perform tasks with superhuman speed and accuracy. She is also a skilled swordsman, able to wield a sword with deadly precision. This dual identity is a constant source of tension in her life, as she struggles to reconcile her love of the Renaissance era with her role in a futuristic, high-tech society.\n",
      "\n",
      "As the day wears on, Aria's work takes her to various parts of the city, from the towering corporate headquarters to the cramped, dimly-\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:50:21.207406188Z",
     "start_time": "2023-09-27T09:50:21.205054198Z"
    }
   },
   "id": "1cc637e821bd17a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e226d2c9cfddd45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3f799193c8d1781"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a large language model? [/INST]  A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. everybody. These models are typically trained on vast amounts of text data, such as books, articles, and websites, and are designed to learn the patterns and structures of language.\n",
      "\n",
      "Large language models are often used in natural language processing (NLP) tasks such as language translation, text summarization, and language generation. They are also used in chatbots, virtual assistants, and other applications where language understanding and generation is required.\n",
      "\n",
      "Some of the key features of large language models include:\n",
      "\n",
      "1. Deep learning architecture: Large language models are typically built using deep learning architectures such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks,\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T09:55:52.005775892Z",
     "start_time": "2023-09-27T09:55:18.892613063Z"
    }
   },
   "id": "43bbc5e1676bcff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72f8dac3a00c71bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5afe8823f560ef14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f1cda18548f6f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "56135b64ccc5ece7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 15:20:15.160929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 15:20:15.697559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b6500adfcc4b3f881bd92a9df9ced4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3636e3a56a644f7c98f922366bd478fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "394e70e3ce9b4fa5816ca8df4edde17b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e91a53c4ed7d4aafb65559d8553c9b12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28c969d11ea64caba8610f0aa43ed68c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b53c3ee016c48dba4470a963d8b71f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/966 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ace872ab4d8148e78ae0cb5130db8997"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4b9cb83da904a64aadc78ca63c17450"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f98586087bb747b49a6943812362cc75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf83079cb02544c1bcc87b451eec1633"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d5d3f9117084b059c4d6c752851f5e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T13:23:39.000642953Z",
     "start_time": "2023-10-06T13:20:14.013280699Z"
    }
   },
   "id": "8628c434538fac21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bee949710129e245"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/fleeb/miniconda3/envs/clones/lib/python3.8/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 386, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space Debris Increase (SDI)\n",
      "Number of Satellites Launched (NSL)\n",
      "Satellite Size (SS)\n",
      "International Space Regulations (ISR)\n",
      "Technological Advancements (TA)\n",
      "Orbital Path Density (OPD)\n",
      "Satellite Functionality (SF)\n",
      "\n",
      "ISR -> NSL: Observable\n",
      "International space regulations influence the number of satellites that can be launched.\n",
      "TA -> NSL: Observable\n",
      "Technological advancements influence the number of satellites that can be launched.\n",
      "NSL -> OPD: Observable\n",
      "The number of satellites launched affects the density of satellites in orbital paths.\n",
      "ISR -> SS: Observable\n",
      "International space regulations influence the size of satellites that can be launched.\n",
      "TA -> SS: Observable\n",
      "Technological advancements influence the size of satellites being made and launched.\n",
      "SS -> SF: Observable\n",
      "The size of satellites influences their functionality.\n",
      "OPD -> SDI: Observable\n",
      "The density of satellites in orbital paths influences the increase in space debris.\n",
      "SF -> SDI: Observable\n",
      "The functionality of satellites influences the increase in space debris.\n",
      "\n",
      "Fill in the following probabilities. For each one make sure to provide a range of reasonable values. For example `0.6-0.8` for an event that we can expect to occur 60%-80% of the time.\n",
      "\n",
      "\n",
      "marginal probability of TA=1: [0.3, 0.5]\n",
      "marginal probability of SS=1: [0.1, 0.12]\n",
      "marginal probability of NSL=1: [0.7, 0.8]\n",
      "marginal probability of ISR=1: [0.1\n"
     ]
    }
   ],
   "source": [
    "out = pipe('''Space Debris Increase (SDI)\n",
    "Number of Satellites Launched (NSL)\n",
    "Satellite Size (SS)\n",
    "International Space Regulations (ISR)\n",
    "Technological Advancements (TA)\n",
    "Orbital Path Density (OPD)\n",
    "Satellite Functionality (SF)\n",
    "\n",
    "ISR -> NSL: Observable\n",
    "International space regulations influence the number of satellites that can be launched.\n",
    "TA -> NSL: Observable\n",
    "Technological advancements influence the number of satellites that can be launched.\n",
    "NSL -> OPD: Observable\n",
    "The number of satellites launched affects the density of satellites in orbital paths.\n",
    "ISR -> SS: Observable\n",
    "International space regulations influence the size of satellites that can be launched.\n",
    "TA -> SS: Observable\n",
    "Technological advancements influence the size of satellites being made and launched.\n",
    "SS -> SF: Observable\n",
    "The size of satellites influences their functionality.\n",
    "OPD -> SDI: Observable\n",
    "The density of satellites in orbital paths influences the increase in space debris.\n",
    "SF -> SDI: Observable\n",
    "The functionality of satellites influences the increase in space debris.\n",
    "\n",
    "Fill in the following probabilities. For each one make sure to provide a range of reasonable values. For example `0.6-0.8` for an event that we can expect to occur 60%-80% of the time.\n",
    "\n",
    "\n",
    "marginal probability of TA=1: [0.3, 0.5]\n",
    "marginal probability of SS=1: [0.1, 0.12]\n",
    "marginal probability of NSL=1: [0.7, 0.8]\n",
    "marginal probability of ISR=1: [0.''')\n",
    "print(out[0]['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T13:35:09.213220224Z",
     "start_time": "2023-10-06T13:34:57.532132477Z"
    }
   },
   "id": "55f7d11324609ee5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b02df37b03797bbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T12:34:19.122084003Z"
    }
   },
   "id": "3c8604fcb18500de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T12:34:19.122187934Z"
    }
   },
   "id": "ed2aeb8bc39bc331"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
